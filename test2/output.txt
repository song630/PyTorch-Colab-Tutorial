Called with args:
Namespace(batch_size=32, checkepoch=1, crop_size=32, cuda=True, decay_step=10,
lr=0.001, max_epochs=40, num_workers=0, resume=False, save_dir='')

net:
SimpleNet(
  (head_net): Sequential(
    (0): NetUnit(
      (conv): Conv2d (3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU()
    )
    (1): NetUnit(
      (conv): Conv2d (32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU()
    )
    (2): NetUnit(
      (conv): Conv2d (32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (norm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU()
    )
    (3): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))
    (4): NetUnit(
      (conv): Conv2d (32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU()
    )
    (5): NetUnit(
      (conv): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU()
    )
    (6): NetUnit(
      (conv): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU()
    )
    (7): NetUnit(
      (conv): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (norm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU()
    )
    (8): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))
    (9): NetUnit(
      (conv): Conv2d (64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU()
    )
    (10): NetUnit(
      (conv): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU()
    )
    (11): NetUnit(
      (conv): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU()
    )
    (12): NetUnit(
      (conv): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU()
    )
    (13): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), dilation=(1, 1))
    (14): NetUnit(
      (conv): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU()
    )
    (15): NetUnit(
      (conv): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU()
    )
    (16): NetUnit(
      (conv): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)
      (relu): ReLU()
    )
    (17): AvgPool2d(kernel_size=4, stride=4, padding=0, ceil_mode=False, count_include_pad=True)
  )
  (fc): Linear(in_features=128, out_features=10)
)


inputs: Variable containing:
(0 ,0 ,.,.) =
 -1.0000  0.6471  0.6627  ...  -0.0980 -0.3490 -0.4039
 -1.0000  0.6706  0.6863  ...  -0.0902 -0.4118 -0.3725
 -1.0000  0.7098  0.7412  ...   0.1608 -0.0902 -0.3098
           ...                           ...
 -1.0000 -1.0000 -1.0000  ...  -1.0000 -1.0000 -1.0000
 -1.0000 -1.0000 -1.0000  ...  -1.0000 -1.0000 -1.0000
 -1.0000 -1.0000 -1.0000  ...  -1.0000 -1.0000 -1.0000

(0 ,1 ,.,.) =
 -1.0000  0.9608  0.9765  ...  -0.1765 -0.3020 -0.3098
 -1.0000  0.9608  0.9765  ...  -0.1059 -0.3333 -0.2549
 -1.0000  0.9765  1.0000  ...   0.0902 -0.1059 -0.2627
           ...                           ...
 -1.0000 -1.0000 -1.0000  ...  -1.0000 -1.0000 -1.0000
 -1.0000 -1.0000 -1.0000  ...  -1.0000 -1.0000 -1.0000
 -1.0000 -1.0000 -1.0000  ...  -1.0000 -1.0000 -1.0000

(0 ,2 ,.,.) =
 -1.0000  0.9843  1.0000  ...  -0.2941 -0.2627 -0.2314
 -1.0000  0.9765  0.9922  ...  -0.2235 -0.3020 -0.1686
 -1.0000  0.9843  1.0000  ...  -0.0510 -0.1608 -0.2078
           ...                           ...
 -1.0000 -1.0000 -1.0000  ...  -1.0000 -1.0000 -1.0000
 -1.0000 -1.0000 -1.0000  ...  -1.0000 -1.0000 -1.0000
 -1.0000 -1.0000 -1.0000  ...  -1.0000 -1.0000 -1.0000


(1 ,0 ,.,.) =
 -0.0431  0.1059 -0.0353  ...  -1.0000 -1.0000 -1.0000
  0.0667  0.1216  0.0588  ...  -1.0000 -1.0000 -1.0000
  0.0667  0.2000  0.0902  ...  -1.0000 -1.0000 -1.0000
           ...                           ...
 -0.5451 -0.4824 -0.4902  ...  -1.0000 -1.0000 -1.0000
 -0.5686 -0.5216 -0.4980  ...  -1.0000 -1.0000 -1.0000
 -0.1686 -0.4667 -0.5294  ...  -1.0000 -1.0000 -1.0000

(1 ,1 ,.,.) =
 -0.2000 -0.0353 -0.1373  ...  -1.0000 -1.0000 -1.0000
 -0.0588 -0.0353 -0.0745  ...  -1.0000 -1.0000 -1.0000
 -0.0588  0.0824 -0.0039  ...  -1.0000 -1.0000 -1.0000
           ...                           ...
 -0.5529 -0.5059 -0.5059  ...  -1.0000 -1.0000 -1.0000
 -0.5765 -0.5216 -0.5059  ...  -1.0000 -1.0000 -1.0000
 -0.4588 -0.5216 -0.5059  ...  -1.0000 -1.0000 -1.0000

(1 ,2 ,.,.) =
 -0.2000 -0.0510 -0.1451  ...  -1.0000 -1.0000 -1.0000
 -0.0745 -0.0353 -0.0824  ...  -1.0000 -1.0000 -1.0000
 -0.0745  0.0667  0.0275  ...  -1.0000 -1.0000 -1.0000
           ...                           ...
 -0.5059 -0.4588 -0.4667  ...  -1.0000 -1.0000 -1.0000
 -0.5373 -0.4824 -0.4588  ...  -1.0000 -1.0000 -1.0000
 -0.4275 -0.4902 -0.4667  ...  -1.0000 -1.0000 -1.0000


(2 ,0 ,.,.) =
 -1.0000 -1.0000 -1.0000  ...  -1.0000 -1.0000 -1.0000
 -1.0000  1.0000  1.0000  ...   1.0000  1.0000  1.0000
 -1.0000  1.0000  0.9922  ...   0.9922  0.9922  0.9922
           ...                           ...
 -1.0000  0.9765  0.9216  ...   0.9529  0.9922  0.9922
 -1.0000  1.0000  0.9843  ...   0.7961  0.9529  0.9843
 -1.0000  1.0000  0.9922  ...   0.9294  0.9765  0.9922

(2 ,1 ,.,.) =
 -1.0000 -1.0000 -1.0000  ...  -1.0000 -1.0000 -1.0000
 -1.0000  1.0000  1.0000  ...   1.0000  1.0000  1.0000
 -1.0000  1.0000  0.9922  ...   0.9922  0.9922  0.9922
           ...                           ...
 -1.0000  1.0000  0.9373  ...   0.9373  0.9765  0.9922
 -1.0000  1.0000  0.9843  ...   0.7961  0.9529  0.9843
 -1.0000  1.0000  0.9922  ...   0.9294  0.9765  0.9922

(2 ,2 ,.,.) =
 -1.0000 -1.0000 -1.0000  ...  -1.0000 -1.0000 -1.0000
 -1.0000  1.0000  1.0000  ...   1.0000  1.0000  1.0000
 -1.0000  1.0000  0.9922  ...   0.9922  0.9922  0.9922
           ...                           ...
 -1.0000  0.9843  0.9294  ...   0.9529  1.0000  1.0000
 -1.0000  1.0000  0.9843  ...   0.7961  0.9529  0.9922
 -1.0000  1.0000  0.9922  ...   0.9294  0.9765  0.9922
...


(29,0 ,.,.) =
 -1.0000 -1.0000 -1.0000  ...  -1.0000 -1.0000 -1.0000
  0.7961  0.7961  0.7882  ...   0.6941 -1.0000 -1.0000
  0.7961  0.7961  0.7882  ...   0.6941 -1.0000 -1.0000
           ...                           ...
 -0.1059 -0.2549 -0.3098  ...   0.5608 -1.0000 -1.0000
 -0.0588 -0.0667  0.0118  ...   0.5137 -1.0000 -1.0000
  0.2000  0.2000  0.2627  ...   0.5059 -1.0000 -1.0000

(29,1 ,.,.) =
 -1.0000 -1.0000 -1.0000  ...  -1.0000 -1.0000 -1.0000
  0.7412  0.7412  0.7412  ...   0.6941 -1.0000 -1.0000
  0.7412  0.7412  0.7333  ...   0.6941 -1.0000 -1.0000
           ...                           ...
 -0.1059 -0.2157 -0.2392  ...   0.4902 -1.0000 -1.0000
 -0.0980 -0.0667  0.0353  ...   0.4431 -1.0000 -1.0000
  0.1294  0.1608  0.2392  ...   0.4275 -1.0000 -1.0000

(29,2 ,.,.) =
 -1.0000 -1.0000 -1.0000  ...  -1.0000 -1.0000 -1.0000
  0.8039  0.7882  0.7569  ...   0.7569 -1.0000 -1.0000
  0.8039  0.7961  0.7882  ...   0.7569 -1.0000 -1.0000
           ...                           ...
 -0.2549 -0.3490 -0.3412  ...   0.4980 -1.0000 -1.0000
 -0.2235 -0.1843 -0.0510  ...   0.4510 -1.0000 -1.0000
  0.0353  0.0667  0.1686  ...   0.4431 -1.0000 -1.0000


(30,0 ,.,.) =
 -1.0000 -1.0000 -1.0000  ...  -1.0000 -1.0000 -1.0000
 -1.0000 -1.0000 -1.0000  ...  -1.0000 -1.0000 -1.0000
 -1.0000 -1.0000 -1.0000  ...  -1.0000 -1.0000 -1.0000
           ...                           ...
  0.1451  0.0980  0.0980  ...   0.2157  0.2235  0.2627
  0.1765  0.1451  0.1843  ...   0.1608  0.1765  0.2471
  0.1765  0.1529  0.1922  ...   0.2314  0.2941  0.2627

(30,1 ,.,.) =
 -1.0000 -1.0000 -1.0000  ...  -1.0000 -1.0000 -1.0000
 -1.0000 -1.0000 -1.0000  ...  -1.0000 -1.0000 -1.0000
 -1.0000 -1.0000 -1.0000  ...  -1.0000 -1.0000 -1.0000
           ...                           ...
  0.0980  0.0510  0.0510  ...   0.1686  0.1765  0.2078
  0.1294  0.0980  0.1373  ...   0.1059  0.1216  0.1922
  0.1294  0.1059  0.1451  ...   0.1608  0.2392  0.2078

(30,2 ,.,.) =
 -1.0000 -1.0000 -1.0000  ...  -1.0000 -1.0000 -1.0000
 -1.0000 -1.0000 -1.0000  ...  -1.0000 -1.0000 -1.0000
 -1.0000 -1.0000 -1.0000  ...  -1.0000 -1.0000 -1.0000
           ...                           ...
  0.0039 -0.0431 -0.0431  ...   0.0588  0.0902  0.1294
  0.0353  0.0039  0.0431  ...   0.0039  0.0431  0.1137
  0.0353  0.0118  0.0510  ...   0.0902  0.1608  0.1294


(31,0 ,.,.) =
 -1.0000 -1.0000 -1.0000  ...  -1.0000 -1.0000 -1.0000
 -0.3961 -0.3569 -0.3333  ...  -0.0118  0.0039 -1.0000
 -0.3412 -0.3176 -0.3020  ...   0.0118  0.0196 -1.0000
           ...                           ...
  0.8510  0.9059  0.7882  ...  -0.1529 -0.3098 -1.0000
  0.7490  0.8431  0.5529  ...  -0.1373 -0.2000 -1.0000
 -0.0902 -0.0039 -0.4275  ...  -0.2000 -0.2078 -1.0000

(31,1 ,.,.) =
 -1.0000 -1.0000 -1.0000  ...  -1.0000 -1.0000 -1.0000
 -0.2549 -0.2078 -0.1608  ...   0.2078  0.2235 -1.0000
 -0.2000 -0.1608 -0.1294  ...   0.2314  0.2392 -1.0000
           ...                           ...
  0.8039  0.8510  0.7490  ...   0.0039 -0.1216 -1.0000
  0.6941  0.7804  0.5216  ...   0.0039 -0.0275 -1.0000
 -0.1529 -0.0745 -0.4431  ...  -0.1059 -0.0902 -1.0000

(31,2 ,.,.) =
 -1.0000 -1.0000 -1.0000  ...  -1.0000 -1.0000 -1.0000
 -0.0039  0.0431  0.0824  ...   0.3804  0.3961 -1.0000
  0.0588  0.0902  0.1137  ...   0.4039  0.4118 -1.0000
           ...                           ...
  0.7647  0.8118  0.7255  ...   0.1059 -0.0196 -1.0000
  0.6471  0.7333  0.5059  ...   0.0980  0.0980 -1.0000
 -0.1529 -0.0667 -0.4039  ...  -0.0196  0.0902 -1.0000
[torch.cuda.FloatTensor of size 32x3x32x32 (GPU 0)]

targets: Variable containing:
correct annotations: (10 classes)
 1, 0, 1, 3, 5, 9, 0, 0, 0, 4, 0, 4, 2, 7, 5, 7,
 4, 3, 7, 9, 6, 4, 5, 7, 8, 9, 5, 9, 4, 9, 0, 0
[torch.cuda.LongTensor of size 32 (GPU 0)]

outputs: Variable containing:

--every row: the output corresponding to one input (also the probabilities of belonging to each category)
--32 columns: every image in a batch

Columns 0 to 7
 -6.1741   9.8957  -5.3579  -6.4577  -9.7606  -7.0231  -4.4279  -7.9658
  2.7452  -4.7638  -0.0579  -0.2188  -3.3305  -2.8139  -2.1373  -4.9225
 -4.5002   7.2831  -4.4272  -5.3188 -10.7547  -6.0593  -6.2923  -7.1966
 -7.7292 -10.2534  -1.0956   5.6486  -3.2318   2.0386   1.0279  -4.0833
 -5.2256  -8.7875  -2.5539   4.3797  -0.3334   2.8860  -0.5719  -1.3243
 -0.5275  -1.0311  -7.9959  -6.7306  -8.3616 -10.5446  -9.0781  -5.0578
  3.9286  -5.6034  -0.5766  -2.9206  -2.0284  -3.8411  -3.8184  -3.4600
  4.1024  -1.9416  -0.7798  -5.0075  -7.1818  -6.8696  -4.2562  -4.5835
  5.1275  -5.9487   0.4169  -2.5398  -3.1876  -4.5596  -3.6115  -5.4926
 -5.1894  -8.5348  -3.9703  -2.7467   8.1034  -3.0928  -4.8664  -4.5314
  2.1634  -2.3641   1.1984  -2.7693  -4.3794  -4.4403  -1.4331  -3.6868
 -4.0322  -5.7351  -2.4133  -1.3797   4.1596  -2.1127  -2.8323  -1.2925
 -2.1487  -5.7533   4.8960  -2.1053  -1.1915  -3.7414  -1.3199  -4.4312
 -4.1341  -3.6789  -2.7252  -2.4856  -1.4301  -1.5901  -3.4950   5.0975
 -5.5222  -5.2436  -1.0770  -1.9134  -3.3000   4.7214  -3.0310  -2.6246
 -5.1438  -5.5873  -5.1036  -2.7839  -2.4932   1.2031  -5.4876   7.9928
 -4.4585  -9.7437  -1.7676  -0.8145   6.1983  -2.1928  -3.5832  -2.6351
 -5.8363  -9.4711   1.5854   3.4634  -0.2189  -0.1277   2.5894  -4.2118
 -1.5518  -3.8138  -0.0818   1.6578  -2.0763   1.2442  -1.3656   0.5895
 -3.3362  -3.5985  -4.5122  -1.8972  -2.1792  -5.0435  -5.2299  -3.0020
 -5.2058  -4.0326  -3.8672  -4.4296  -5.2115  -4.9383   7.9819  -7.0594
 -5.5138  -8.2582   0.3888   1.8221   1.8474   1.3188   0.0900  -0.0352
 -6.3734  -4.3937  -2.3214  -0.4348  -4.8595   6.1020  -3.6857  -4.1592
 -4.4137  -5.0529  -3.4111  -2.4283  -2.1593   0.3158  -3.9179   6.4849
 -1.0391  -2.5226  -3.3320  -4.9537  -7.1732  -7.1319  -4.4097  -6.0223
 -1.6917   0.6752  -3.7908  -1.8873  -5.7288  -2.3239  -3.7402  -2.1569
 -3.3418  -3.3854  -1.4359   1.6394  -4.1295   3.2146  -0.8005  -1.2178
 -1.8037   3.9477  -8.7114  -7.9534 -11.3150  -9.5459 -10.4992  -6.7561
 -3.8523  -6.1177  -2.3549  -1.7028   5.8800  -2.8260  -3.4081  -2.7565
  1.9122   1.5570  -6.8547  -4.5330 -11.2584  -7.5972 -10.6482  -6.0641
  3.6110  -4.3238  -1.1838  -0.1090  -3.8257  -2.4658  -4.4432  -4.9309
 -1.5035  -1.6531   2.5229  -1.4417  -3.6193  -3.9071  -3.5402  -1.0787

Columns 8 to 9
 -3.0512  -2.7845
 -2.9266  -4.0683
  0.8010   1.6737
 -7.3604  -9.9578
 -7.8706  -7.1146
 -1.7529   9.2508
 -3.2703  -3.1660
  0.6804  -0.5978
 -4.8190  -5.4382
 -8.1052  -7.2377
 -0.0821  -3.4929
 -5.1079  -3.1809
 -3.1159  -5.5131
 -3.7585  -4.0566
 -4.6052  -7.0480
 -7.3947  -5.8057
 -6.5152  -5.7412
 -7.0289  -7.8139
 -4.2743  -4.2797
 -0.2243   3.8060
 -3.5647  -6.0166
 -5.9626  -6.8625
 -5.7591  -6.7270
 -5.1638  -5.3916
  6.7427  -1.3157
 -2.8715   3.1662
 -5.1701  -5.0795
 -1.7392  10.1839
 -6.2265  -5.2736
  3.6611   4.6613
 -5.2127  -3.9860
 -1.8613  -2.1057
[torch.cuda.FloatTensor of size 32x10 (GPU 0)]

loss: Variable containing:
 0.3396
[torch.cuda.FloatTensor of size 1 (GPU 0)]

inputs.size(0): 32
loss.cpu(): Variable containing:
 0.3396
[torch.FloatTensor of size 1]

loss.cpu().data:
 0.3396
[torch.FloatTensor of size 1]

outputs.data:
[torch.cuda.FloatTensor of size 32x10 (GPU 0)]

prediction:
 1, 0, 1, 3, 3, 9, 0, 0, 0, 4, 0, 4, 2, 7, 5, 7,
 4, 3, 3, 9, 6, 4, 5, 7, 8, 9, 5, 9, 4, 9, 0, 2
[torch.cuda.LongTensor of size 32 (GPU 0)]
